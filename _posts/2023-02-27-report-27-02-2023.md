---
title: "Report 27/02/2023"
date: 2023-02-27T00:00-00:00
last_modified_at: 2023-02-27T00:00-00:00
categories:
  - code
  - deep learning
permalink: /report-27-02-2023/
classes: wide
excerpt: Report

---
### ATF
Bài báo ["An Attention Free Transformer"](https://arxiv.org/abs/2105.14103) có những chi tiết sau:

- Bài báo giới thiệu một kiến trúc mới cho Transformer gọi là AFT (Attention Free Transformer), mà không cần sử dụng self-attention như trong Transformer gốc.
- AFT sử dụng một phép toán mới gọi là element-wise product attention (EPA), mà thay vì tính tích vô hướng giữa query và key như trong self-attention, thì tính tích vô hướng giữa query và key cộng với một bias vị trí học được.
- EPA có độ phức tạp bộ nhớ tuyến tính với cả kích thước ngữ cảnh và số chiều của vector đặc trưng, trong khi self-attention có độ phức tạp bậc hai với kích thước ngữ cảnh và tuyến tính với số chiều vector đặc trưng.
- Bài báo so sánh hiệu năng của AFT và Transformer trên các nhiệm vụ xử lý ngôn ngữ tự nhiên (NLP) và nhận diện hình ảnh (CV), bao gồm: dịch máy, phân loại văn bản, sinh ảnh từ chú thích, phát hiện đối tượng và phân đoạn ảnh.
- Kết quả thực nghiệm cho thấy AFT có thể đạt được kết quả tương đương hoặc vượt trội hơn so với Transformer trên các nhiệm vụ NLP và CV, trong khi tiết kiệm được nhiều bộ nhớ hơn.

### Training MLM for GPT
Bài báo ["Efficient Training of Language Models to Fill in the Middle"](https://arxiv.org/abs/2207.14255) chi tiết:
- Bài báo sử dụng một phương pháp gọi là middle masking (MM), mà xóa một đoạn văn ngẫu nhiên từ giữa văn bản và yêu cầu mô hình điền vào chỗ trống đó. MM khác với span masking (SM), mà xóa nhiều đoạn văn ngắn ở nhiều vị trí trong văn bản và yêu cầu mô hình điền vào tất cả các chỗ trống đó.
- Bài báo cho rằng MM có thể giúp các mô hình ngôn ngữ tự nhiên học được cách sử dụng ngữ cảnh hai chiều để sinh ra văn bản có ý nghĩa và liên quan đến phần đã cho. MM cũng có thể giúp các mô hình ngôn ngữ tự nhiên học được cách hiểu được ý chính của văn bản và tạo ra những câu chuyện mới từ ý chính đó.
- Bài báo huấn luyện các mô hình ngôn ngữ tự nhiên như GPT-2, GPT-3 và T5 trên các tập dữ liệu như C4, BookCorpus và Wikipedia. Bài báo so sánh hiệu suất của các mô hình khi huấn luyện chỉ với SM hoặc chỉ với MM hoặc kết hợp SM và MM ở các tỷ lệ khác nhau.
- Kết quả thực nghiệm cho thấy việc huấn luyện các mô hình chỉ với MM hoặc kết hợp SM và MM ở tỷ lệ cao (80% hoặc 90%) có thể cải thiện hiệu suất của các mô hình trên các nhiệm vụ như sinh câu tiếp theo, sinh câu kết thúc, sinh câu giữa hai câu đã cho, sinh tiêu đề cho bài viết, sinh tóm tắt cho bài viết và trả lời câu hỏi.
